
#+TITLE: Preevyit Clojure

* Synopsis

This is an ad-hoc examination of some extant clojure code as mentioned
at
https://www.reddit.com/r/Clojure/comments/cb02xs/datascriptrum_author_goodbye_clojure_same_loc_17x/
and
https://www.reddit.com/r/Clojure/comments/cb02xs/datascriptrum_author_goodbye_clojure_same_loc_17x/etf6nep/
with the original repository at https://github.com/tonsky/icfpc2019
with a rust version at https://github.com/tonsky/icfpc2019-rust .

I was curious to see what kind of starting point the code base would have,
and whether some relatively low-hanging performance techniques would be
applicable to get any improvement.  So far, the results have been decent.

On my machine, an 8-core i7-6700 HQ 2.6GHZ, with 16 GB of RAM on
Windows 10, Java 1.8 64 bit, and no changes to the original project
options (e.g. default 4GB max heap, default GC, main uses 8 threads),
I got the following results running icpfc.main/main-elided in the
(slightly modified) baseline branch:

#+BEGIN_EXAMPLE
859915ms Left 8 Solved prob-286 {:score 7932, :time 237700} was [elided]
884298ms Left 7 Solved prob-291 {:score 8883, :time 164158} was [elided]
907557ms Left 6 Solved prob-297 {:score 4490, :time 51442} was [elided]
912395ms Left 5 Solved prob-293 {:score 9953, :time 159562} was [elided]
918182ms Left 4 Solved prob-294 {:score 9694, :time 123444} was [elided]
950599ms Left 3 Solved prob-295 {:score 7670, :time 118231} was [elided]
960923ms Left 2 Solved prob-298 {:score 7142, :time 92654} was [elided]
970510ms Left 1 Solved prob-299 {:score 10521, :time 88154} was [elided]
983865ms Left 0 Solved prob-300 {:score 8879, :time 109354} was [elided]
DONE in 983866  ms
#+END_EXAMPLE

After spending some time exploring (really learning the code base), 
benchmarking, and applying incremental operational improvements,
as of 16 July 2019, the version in the `opt` branch, running the
same function gives us:

#+BEGIN_EXAMPLE
98805ms Left 11 Solved prob-286 {:score 7932, :time 17262} was [elided]
99117ms Left 10 Solved prob-292 {:score 8941, :time 9325} was [elided]
102913ms Left 9 Solved prob-289 {:score 10288, :time 13075} was [elided]
102976ms Left 8 Solved prob-296 {:score 3348, :time 3515} was [elided]
104022ms Left 7 Solved prob-291 {:score 8883, :time 13732} was [elided]
105662ms Left 6 Solved prob-297 {:score 4490, :time 5015} was [elided]
109255ms Left 5 Solved prob-293 {:score 9953, :time 13185} was [elided]
109271ms Left 4 Solved prob-294 {:score 9694, :time 9950} was [elided]
112223ms Left 3 Solved prob-298 {:score 7142, :time 9122} was [elided]
112786ms Left 2 Solved prob-295 {:score 7670, :time 9216} was [elided]
114957ms Left 1 Solved prob-300 {:score 8879, :time 9014} was [elided]
115395ms Left 0 Solved prob-299 {:score 10521, :time 7499} was [elided]
DONE in 115395  ms
#+END_EXAMPLE

Or about a +6.91+ `8.52` fold improvement (assuming everything is still
computing correctly, and timing is accurate), or down from ~16m 22s
wall-clock to +2m 21s+ 1m 54s.  Comparisons of the baseline scores (in baseline
branch, scores.edn) and the resulting scores (in opt branch,
scores.edn) show problems have identical scores.

I think there are more tweaks and hacks to be applied.  Notably,
I did nothing to change the implementation of any search algorithms,
only to speed up existing functions, optimize data access patterns,
and leverage features like inlining and macros to help keep things
both expressive and performant.  I extended the mutable structures
used in the original implementation, but retained persistent structures
as well (e.g. I didn't inject mutability beyond the original semantics).

This has been an interested experiment, and looks like it will
continue tobe going forward.  There more tweaks to try (hopefully
while maintaining expressivess).  I don't know if we'll overtake Rust or
a systems level language, but I'd like to see how close we can get
relative to effort spent / distortion of Clojure (e.g. writing mutable
array-based Java in Clojure).

Perhaps the most striking feature is that - despite working with
unknown legacy code, and having a limited amount of time to hack
on this in my off hours, there were indeed some reasonable performance
tweaks, some of which (like the emergent with-slots macro) led
to serious gains without sacrificing the legacy expressability.
On the other hand, the original code base was sufficiently
well written and more-or-less idiomatic Clojure, to the point
identifying expensive idioms and factoring them into performant
constructs also proved fairly straightforward (with the help
of a decent profiler and some pre-existing rules of thumb...).

* Acknowledgement

Thanks to the authors of the original implementation for sharing
their work and allowing me to learn from it (in particular
Nikita Prokopov, a.k.a. Tonsky).

* Main Impediments

** hash comparison for Points is inefficient
The default hash function is suboptimal for the data.  Rust does not
pay this penalty, we should aim to avoid it.

** Points used everywhere incur hash debt
Authors leverage Point records all over, including insertion and 
lookup from associative containers.  Faster hashing means
faster reads, etc.  For hot paths, this is pretty effective
(initially just improving Points took the problem runtime
from 100s to ~20s on my machine for the "big" problem instance
used for testing, prob-277.).

** Precomputed lookup table is inefficient
Again, this leverages Points for everything.  There are
also commonalities in the data (e.g. all x coordinates
are 1) that allow for more efficient lookup schemes
at little cost in expressivity.  Big payoff in practice.

** State is maintained in a PersistentHashMap
Given the amount of times fields are accessed, the cost is
unacceptable.  Far better to optimize for reads and use records or
types.  I opted to use defrecords for level state and leverage field
access as much as possible.  This gets us closer (but not exactly)
to Rust's implementation that uses Structs, again without
losing expressiveness.

Note: this may not hold in practice for ArrayMaps, since the 
same performance windfall for the level representation
did not seem to carry over for instance to the robot (for now).

** nth and get via destructuring, on hot paths
Initially unrolled hot spots (via presence of time spent in
clojure.lang.RT.nth and clojure.lang.RT.get for vector/seq and map
destructuring respectively.

After unrolling and getting performance stable, I decided to revisit
the original intent the authors' had for leveraging Clojure's
expressivenes, while balancing optional performance optimizations.
icfpc.core.speed/with-slots is a macro that acts similar to a let
binding, but specializes in type-hinted destructuring (to include
fields for records, deftypes, and arbitrary classes).

*** icfpc.speed/with-slots
Allows for efficient, type-based destructuring similar to the
idiomatic destructuring forms of Clojure, with some limitations.
Bindings are presented as the typical vector, with an even number of
entries, where the preceding odd binding establishes binds for the
even successor.  Unlike typical forms, bindings leverage
type-hinting information - both on the left hand side and the right
hand side - to establish efficient operations beyond the generic
destructuring forms established with maps and vectors, e.g. get and
nth.

Callers may use {:fields [a b ^clojure.lang.Counted c] }, along with
a type-hinted rhs, to denote establishing bindings for a, b, c, by
invoking like-named direct, type-hinted field applications on the
rhs, ala (.a ^some-type rhs).

Any binding var hinted on the LHS will propogate its hint throughout
later bindings.  This allows an expressive form of efficient
destructuring for the consenting adult, which allows idiomatic
expressivity without the accompanying significant loss of
performance.

map destructuring for {:keys [...]} follows that of :fields, except
the bindings are established via either a (.valAt ..) or (.get ..)
or (get ...) depending on the presented type, get being the fallback.
This allows usage with types supporting the java.util.Map interface.
Literal maps are automatically inferred with efficient getters.

Vector or indexed destructuring is similarly supported,
[^some-type x y] ^clojure.lang.Indexed coll will invoke efficient
.nth indexing operations rather than the slower, more general nth.
Depending on the presented type, either .nth, .get, or nth will be
used, allowing operation with structures supporting the
java.util.List interface.  Literal vectors are automatically
inferred with efficient getters.  The & rest notation is currently
NOT supported...

The remaining rules act identically to let semantics.  If a symbol
is bound to the LHS, then the binding is passed through
untouched (including hints).

with-slots tries to scan the input bindings to find
discrepancies (such as duplicate binds), and to re-use existing
hinted information for binds.  In the case that the user decides to
re-hint a RHS var that has already been hinted a-priori, with-slots
will allow the hint for that binding, but revert to prior hinting
unless the user continues to specify new hints.  This seems rare in
practice.

It's common to import the symbols for the
[clojure.lang Counted Indexed] interfaces when using with-slots.

An example:

#+BEGIN_SRC clojure  
(with-slots
  [{:fields [^Counted path
             ^Indexed position]} ^botmove (->botmove [] [1 2])
   {:keys [a b] :fields [hashCode]}    {:a 2 :b 3}
   [x y]          position         
   path-length   (.count path)]
 [hashCode (+ x y)])
#+END_SRC

This provided a way to tune performance without deviating too far from
Clojure idioms, and provides warnings when the caller is entering a
slow path (e.g. causing a function call to get or nth).  It's
basically a poor man's optimizing compiler for the use-case of
unpacking type-hinted structures for efficient reads.
 
** Dynamic var lookups incur deref costs, significant on hot paths..
This one was rough, since the original design leverages dynamic
vars everywhere stylistically.  I either disabled them and folded
their contribution into the level state (along with quick access) since
the level is pushed around just about everywhere, or I cached the
result of the currently bound value 1x before doing any signifcant
work (e.g. in icfpc.bot.explore*).  This is just basic stuff where
you stop doing repeated work, e.g. move it outside of your iterating
code and hot paths.

** Search Fringe for explore improved efficiency
Surprisingly enough, it's pretty tough to beat the java.util.HashSet 
implemented search fringe.  I tried a couple of different backends,
including bifurcan's LinearSet, IntMap, etc.  I eventually
settled on (based on the size of the maps, estimated at 400 x 400 from
the data), using a dense representation behing an IFringe protocol.
This protocol wraps the search fringe, providing access (e.g.
our precious direct method invocation).  The backing store operates
as before (caching Points' [x y] coordinates to determine containment),
but it leverages a dense 2d boolean array and avoids hashing entirely.
This proved to be a significant performance boost, since checking
containment and pushing items on the fringe happens A LOT.

** 1D array access projecting from 2 coords arithmetically appears slower then 2d array
Wrapped the byte arrays (currently) in a facade implememting an
IByteMap interface.  Better perf, also a simpler API to work with.

** Some function calls occur frequently enough to benefit from inlining
A little added noise, but definline useful 

** Some numerical ops benefit from explicit operators, e.g. == and unchecked stuff
Minor benefit but easy.  This wasn't crushing us, but helped on the
margins.  Datastructure access was far more important...

** Avoid RestFn invocation due to varargs arities
Numeric comparisons like <, when invoked with arities above 2, resort to 
a varargs implementation.  The idiom (< -1 x y) showed up in many places
on the hot path.  Every time you do this, you incur a small penalty,
as the destructured args reprsented by the xs as in (fn [x y & xs] ) 
are coerced into a seq and require some additional allocation and
function calls.  If you're on a hot path, and doing this in multiple
places, it adds up.  A quick fix, particularly for inlined calls (common
in this setting), is to just define a macro or equivalent inline function
that eliminates the need for varargs.
** Comparative version in Rust appears to not use compare solutions, so we don't
elided the call to compare solutions, available via [icfpc.main
solve-elided main-elided].

It "looks" like the Rust implementation is also far lighter in what it's
doing in some other areas (e.g. Thread checks and other stuff).  Likely
some more performance gains after doing a comparative review of the code (or 
better yet, just port the simpler Rust version using newfound tools and principles).

** Recomputing jump points inside loop, jump points are sparse too.
Moved this out of the loop, into a delayed value that is only computed
once and re-used.

** Lots of calls to update and variadic assoc
Updating a nested map requires at least n calls to get/.valAt read the
map you're trying to update, one function call (possibly variadic)
to apply to the entry, then another n calls to assoc or .assoc to
pack the map back together.

The legacy implementation does a lot of single-key updates inside of
functions on hot paths, when the multiple lookups could be trivially
flattened into a single lookup, an operation on the map to update
multiple entries efficiently, and a single assoc.
We still pay the price for multiple lookups/assoces, but we can
remove up to n redundant get/assoc pairs and get some big wins
without losing expressiveness.

*** assoc*
The initial idea here is to flatten the aforementioned update(s)
into a single update (icfpc.core/map-bot), and apply a function
that efficiently does the work therein.  The naive solution is
a combination of `with-slots` for efficient field/key access,
and a simple variadic call to `assoc` to update multiple keys
simultaneously.  This, however, is suboptimal due to
the variadic call to assoc.  We'd like to preserve the
nice nature of supply multiple arguments, but invoke
individual calls to assoc (avoiding variadic fn invocation).

This leads to a simple macro `assoc*` which helps us along.
On the large map instance, this alone netted about a 6% increase
for modifying a single function `icfpc.bot/move`.

* Pending Efforts

** Load Level
I spent a decent amount of time exploring optimizations here, and implemented
many.  This isn't the lion's share of the performance cost though, but it's
on par with some of the bigger functions inside the bot namespace.  There are
likely additional optimizations.

** General Transition to Hinted Destructuring via with-slots
There are several functions I haven't transition to, that still
leverage the default get and nth implementations for destructuring
and ignore direct field access of level state.  I'm planning to
get them folded in and benchmark. 

** Verification of Results
To date, all of the transforms have been operational, rather than semantic.
The results "should" match the clojure baseline.  During testing, primarily
against the "big" problem set prob-277, I kept an eye on maintaining the
same scores throughout, and did catch one regression but fixed it.

Results currently check out when comparing the baseline scores.edn 
and the opt branch's scores.edn (each from respective run output.

I am confident the opt implementation is functionally equivalent to
the original baseline implementation.

** Additional performance
Aside from the basic mechanical changes (e.g. efficient reads), I
didn't alter anything algorithmically, since I didn't spend any time
reading about the original problem or comparing the implementation's
approach.  Consequently, there are likely additional areas to tweak,
introduce local mutation, or even leverage parallelism.  Currently the
distribution of work is effectively coarse-grained, on a per-problem
basis.  I suspect dissecting the work into finer-grained units for
integration with a producer/consumer queue (or a similar method) would
yield some nice results.  In other words, there may be additional
techniques that Clojure can explore to exploit available resources.

Some of the existing implementation is a product of experimentation.
The PooledFringe is - in hindsight - unnecessary if one just packs
along a search fringe with the level.  This allows workers to 
use (or re-use) the fringe instead of the current implementation
that maps threads to their own fringe via a concurrent hash map.
That introduces some needless overhead.

Another lateral would be going even more primitive in the data
representation and leveraging byte-buffer backed implementations
like tech.datatype or other struct-like libraries (Zach Tellman has
a couple).  These could open some interesting possibilites for
macros, cache local arrays, etc.  It would be an interesting
challenge to introduce psuedo value types via Clojure macros 
without losing expressiveness.

* Working Notes...
** BenchMarking Oddities
I started noticing weird random performance regressions that didn't
make sense, beyond the typical benchmarking noise.

Looks like something going on at the OS / JVM level for some reason,
perhaps virus scanning junk, etc. (W10).  If you run into
odd stuff, it could be something exogenous...

After restarting, I get the same project spinning back up hitting
records again.  Weird, and no idea what's causing it.
** Interesting Perf Difference Between ArrayMaps and Records
Original idea of just "structing" the bot representation
into a record and getting the same benefits from
field access turned out to be counterintuitive in practice.


Strangely, arraymaps "appear" to maintain a slight
edge, since bot only has like 6 keys.  This doesn't
make a ton of sense, in that field lookups should
dominate, even for arraymaps.  It looks like if the
keys are in the front, you get some linear
fast access for the arraymap akin to a field lookup.
Strange, still on nanosecond scale...

**** ArrayMaps
- This difference stems in how function calls are invoked, and
  indirectly the implementation of .valAt
- ArrayMaps, i.e. maps with key cardinality <= 8, 
  have an object array representing the keys and vals.
- valAt for arraymaps just strides through each key slot,
  basically an array lookup, and checks identical? for the
  input key (object has a similar path, but a bit more expensive).
- the IFn implementation for arraymaps delegates to .valAt.

****  Records....
- Records have this notion that they're going to be uber fast
  for their static or canonical keys, since the keys are actually
  object fields.

- valAt is implemented as a case dispatch based on the input key
  to see if it's a static key, which is returned immediately,
  otherwise .valAt is invoked on the embedded hashmap for
  non-canonical keys.

- This is pretty efficient in most cases, particualry in comparison
  to PersistentHashMaps (not ArrayMaps), where simple lookups
  are waaaay faster, and field lookups are 10x faster.

- HOWEVER, the implementation of 'case doesn't compete with
  the naive array-based lookup strategy of ArrayMap...

- ArrayMap is just traversing an array of ints (object pointers)
  and checking for equality (identical?).  This implementation
  has the nice property that it can be faster for keys earlier
  in the collection, and in general (up to the 8th key) is 
  faster than the case-based dispatch (which I assume requires some
  kind of lower level hashing).

- The net implication is that compared to arraymaps, key lookup
  is on average SLOWER for records, despite records having
  access to static fields!

- On top of this, the record implementation has no default IFn
  implementation, so it's not invokable...

- My goal in designing defrecord+ is to alleviate this, to
  allow records to be used interchangeably with arraymaps
  and retain the performance, while allowing use of field accesses
  as an optimization (via with-slots).

** Interesting perf difference with clojure.core/update
Did some exploratory macro inlining, along with
leveraging field access on bots, etc. For some
reason, current metrics indicate  that the 
stock clojure.core/update function is just fine,
if not slightly dominant during a whole-program
run.  Microbenchmarks indicate going the macro
route is dominant, but in practice (perhaps
due to inlining...) this isn't so.

** Unexplained phenomena with unchecked math
I added some unchecked math ops early on prior to more thorough
profiling and exploration, which affected 2 functions in particular:
icfpc.level/valid-hand?  and icfpc.level/obstacle? .  

There was originally just one function (valid-hand?)  which I split
out into two during code spelunking (may reverse that decision in the
future).  It turns out that using unchecked math there for the
arithmetic actually introduced a regression, where the baseline
results did not match the opt branch.  I missed this regression (as it
happened before I locked things down for testing), but managed to find
and fix it prior to publishing this repo.  

So...word of wisdom: make sure your operations really can use
unchecked math :) I have no idea why in this case they don't (since
they're ostensibly integers...).

** dynamic var lookups
 (nth bots *bot*)
 
** rate function, explore*
** lots of destructuring
** advance*
*** ifpc.bot/explore*
**** rate
**** valid-hand?
The every? predicate for valid-hand invokes
ifpc.core/get-level 

get-level is primarily doing map lookups via
keyword lookup sites, could be an opportunity
to optimize.

coord->index is invoked a lot as well, causing
some overhead.

Lots of map-as-function invocations.
Lots of RestFN invocations surrounding < being
called on variadic args, leading to seq version
of next.

And not=, but does so against a var without
hinting, should be a number comparison.  Boxed
math..

**** clojure.lang.util/equiv
vector equality, pcequiv
tons of boxed numeric equality...
**** ifpc.core/get-level

**** ifpc.core/coord->idx
     

**** hashset.contains
**** ifpc.core.point/equals
uses map equality
invokes seq implementation
invokes point.size, which invokes clojure.lang.rt/count


**** ifpc.bot/step
    
**** self-time (invocation)
**** clojure.rt.count
**** keyword lookup site / get
**** hashset.add


*** ifpc.bot/act
*** ifpc.bot/move
**** ifpc.level/mark-wrapped
**** ifpc.level/extra-move
     
