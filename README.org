
#+TITLE: Preevyit Clojure

* Synopsis

This is an ad-hoc examination of some extant clojure code as mentioned
at
https://www.reddit.com/r/Clojure/comments/cb02xs/datascriptrum_author_goodbye_clojure_same_loc_17x/
and
https://www.reddit.com/r/Clojure/comments/cb02xs/datascriptrum_author_goodbye_clojure_same_loc_17x/etf6nep/
with the original repository at https://github.com/tonsky/icfpc2019
with a rust version at https://github.com/tonsky/icfpc2019-rust .

I was curious to see what kind of starting point the code base would have,
and whether some relatively low-hanging performance techniques would be
applicable to get any improvement.  So far, the results have been decent.

On my machine, an 8-core i7-6700 HQ 2.6GHZ, with 16 GB of RAM on
Windows 10, Java 1.8 64 bit, and no changes to the original project
options (e.g. default 4GB max heap, default GC, main uses 8 threads),
I got the following results running icpfc.main/main-elided in the
(slightly modified) baseline branch:

#+BEGIN_EXAMPLE
859915ms Left 8 Solved prob-286 {:score 7932, :time 237700} was [elided]
884298ms Left 7 Solved prob-291 {:score 8883, :time 164158} was [elided]
907557ms Left 6 Solved prob-297 {:score 4490, :time 51442} was [elided]
912395ms Left 5 Solved prob-293 {:score 9953, :time 159562} was [elided]
918182ms Left 4 Solved prob-294 {:score 9694, :time 123444} was [elided]
950599ms Left 3 Solved prob-295 {:score 7670, :time 118231} was [elided]
960923ms Left 2 Solved prob-298 {:score 7142, :time 92654} was [elided]
970510ms Left 1 Solved prob-299 {:score 10521, :time 88154} was [elided]
983865ms Left 0 Solved prob-300 {:score 8879, :time 109354} was [elided]
DONE in 983866  ms
#+END_EXAMPLE

After spending some time exploring (really learning the code base), 
benchmarking, and applying incremental operational improvements,
as of 16 July 2019, the version in the `opt` branch, running the
same function gives us:

#+BEGIN_EXAMPLE
122545ms Left 11 Solved prob-286 {:score 7932, :time 22632} was [elided]
122861ms Left 10 Solved prob-292 {:score 8941, :time 12297} was [elided]
126084ms Left 9 Solved prob-289 {:score 10288, :time 17094} was [elided]
127257ms Left 8 Solved prob-296 {:score 3348, :time 4550} was [elided]
128402ms Left 7 Solved prob-291 {:score 8883, :time 17723} was [elided]
131036ms Left 6 Solved prob-297 {:score 4490, :time 6515} was [elided]
133705ms Left 5 Solved prob-293 {:score 9953, :time 17547} was [elided]
135780ms Left 4 Solved prob-294 {:score 9694, :time 12912} was [elided]
138774ms Left 3 Solved prob-298 {:score 7142, :time 11703} was [elided]
138783ms Left 2 Solved prob-295 {:score 7670, :time 12900} was [elided]
141929ms Left 1 Solved prob-299 {:score 10521, :time 10574} was [elided]
142385ms Left 0 Solved prob-300 {:score 8879, :time 11999} was [elided]
DONE in 142385  ms
#+END_EXAMPLE

Or about a 6.91 fold improvement (assuming everything is still
computing correctly, and timing is accurate), or down from ~16m 22s
wall-clock to 2m 21s.  Comparisons of the baseline scores (in baseline
branch, scores.edn) and the resulting scores (in opt branch,
scores.edn) show problems have identical scores.

I think there are more tweaks and hacks to be applied.  Notably,
I did nothing to change the implementation of any search algorithms,
only to speed up existing functions, optimize data access patterns,
and leverage features like inlining and macros to help keep things
both expressive and performant.  I extended the mutable structures
used in the original implementation, but retained persistent structures
as well (e.g. I didn't inject mutability beyond the original semantics).

This has been an interested experiment, and looks like it will
continue tobe going forward.  There more tweaks to try (hopefully
while maintaining expressivess).  I don't know if we'll overtake Rust or
a systems level language, but I'd like to see how close we can get
relative to effort spent / distortion of Clojure (e.g. writing mutable
array-based Java in Clojure).

Perhaps the most striking feature is that - despite working with
unknown legacy code, and having a limited amount of time to hack
on this in my off hours, there were indeed some reasonable performance
tweaks, some of which (like the emergent with-slots macro) led
to serious gains without sacrificing the legacy expressability.
On the other hand, the original code base was sufficiently
well written and more-or-less idiomatic Clojure, to the point
identifying expensive idioms and factoring them into performant
constructs also proved fairly straightforward (with the help
of a decent profiler and some pre-existing rules of thumb...).

* Acknowledgement

Thanks to the authors of the original implementation for sharing
their work and allowing me to learn from it (in particular
Nikita Prokopov, a.k.a. Tonsky).

* Main Impediments

** hash comparison for Points is inefficient
The default hash function is suboptimal for the data.  Rust does not
pay this penalty, we should aim to avoid it.

** Points used everywhere incur hash debt
Authors leverage Point records all over, including insertion and 
lookup from associative containers.  Faster hashing means
faster reads, etc.  For hot paths, this is pretty effective
(initially just improving Points took the problem runtime
from 100s to ~20s on my machine for the "big" problem instance
used for testing, prob-277.).

** Precomputed lookup table is inefficient
Again, this leverages Points for everything.  There are
also commonalities in the data (e.g. all x coordinates
are 1) that allow for more efficient lookup schemes
at little cost in expressivity.  Big payoff in practice.

** State is maintained in a PersistentHashMap
Given the amount of times fields are accessed, the cost is
unacceptable.  Far better to optimize for reads and use records or
types.  I opted to use defrecords for level state and leverage field
access as much as possible.  This gets us closer (but not exactly)
to Rust's implementation that uses Structs, again without
losing expressiveness.

** nth and get via destructuring, on hot paths
Initially unrolled hot spots (via presence of time spent in
clojure.lang.RT.nth and clojure.lang.RT.get for vector/seq and map
destructuring respectively.

After unrolling and getting performance stable, I decided to revisit
the original intent the authors' had for leveraging Clojure's
expressivenes, while balancing optional performance optimizations.
icfpc.core.speed/with-slots is a macro that acts similar to a let
binding, but specializes in type-hinted destructuring (to include
fields for records, deftypes, and arbitrary classes).

*** icfpc.speed/with-slots
Allows for efficient, type-based destructuring similar to the
idiomatic destructuring forms of Clojure, with some limitations.
Bindings are presented as the typical vector, with an even number of
entries, where the preceding odd binding establishes binds for the
even successor.  Unlike typical forms, bindings leverage
type-hinting information - both on the left hand side and the right
hand side - to establish efficient operations beyond the generic
destructuring forms established with maps and vectors, e.g. get and
nth.

Callers may use {:fields [a b ^clojure.lang.Counted c] }, along with
a type-hinted rhs, to denote establishing bindings for a, b, c, by
invoking like-named direct, type-hinted field applications on the
rhs, ala (.a ^some-type rhs).

Any binding var hinted on the LHS will propogate its hint throughout
later bindings.  This allows an expressive form of efficient
destructuring for the consenting adult, which allows idiomatic
expressivity without the accompanying significant loss of
performance.

map destructuring for {:keys [...]} follows that of :fields, except
the bindings are established via either a (.valAt ..) or (.get ..)
or (get ...) depending on the presented type, get being the fallback.
This allows usage with types supporting the java.util.Map interface.
Literal maps are automatically inferred with efficient getters.

Vector or indexed destructuring is similarly supported,
[^some-type x y] ^clojure.lang.Indexed coll will invoke efficient
.nth indexing operations rather than the slower, more general nth.
Depending on the presented type, either .nth, .get, or nth will be
used, allowing operation with structures supporting the
java.util.List interface.  Literal vectors are automatically
inferred with efficient getters.  The & rest notation is currently
NOT supported...

The remaining rules act identically to let semantics.  If a symbol
is bound to the LHS, then the binding is passed through
untouched (including hints).

with-slots tries to scan the input bindings to find
discrepancies (such as duplicate binds), and to re-use existing
hinted information for binds.  In the case that the user decides to
re-hint a RHS var that has already been hinted a-priori, with-slots
will allow the hint for that binding, but revert to prior hinting
unless the user continues to specify new hints.  This seems rare in
practice.

It's common to import the symbols for the
[clojure.lang Counted Indexed] interfaces when using with-slots.

An example:

#+BEGIN_SRC clojure  
(with-slots
  [{:fields [^Counted path
             ^Indexed position]} ^botmove (->botmove [] [1 2])
   {:keys [a b] :fields [hashCode]}    {:a 2 :b 3}
   [x y]          position         
   path-length   (.count path)]
 [hashCode (+ x y)])
#+END_SRC

This provided a way to tune performance without deviating too far from
Clojure idioms, and provides warnings when the caller is entering a
slow path (e.g. causing a function call to get or nth).  It's
basically a poor man's optimizing compiler for the use-case of
unpacking type-hinted structures for efficient reads.
 
** Dynamic var lookups incur deref costs, significant on hot paths..
This one was rough, since the original design leverages dynamic
vars everywhere stylistically.  I either disabled them and folded
their contribution into the level state (along with quick access) since
the level is pushed around just about everywhere, or I cached the
result of the currently bound value 1x before doing any signifcant
work (e.g. in icfpc.bot.explore*).  This is just basic stuff where
you stop doing repeated work, e.g. move it outside of your iterating
code and hot paths.

** Search Fringe for explore improved efficiency
Surprisingly enough, it's pretty tough to beat the java.util.HashSet 
implemented search fringe.  I tried a couple of different backends,
including bifurcan's LinearSet, IntMap, etc.  I eventually
settled on (based on the size of the maps, estimated at 400 x 400 from
the data), using a dense representation behing an IFringe protocol.
This protocol wraps the search fringe, providing access (e.g.
our precious direct method invocation).  The backing store operates
as before (caching Points' [x y] coordinates to determine containment),
but it leverages a dense 2d boolean array and avoids hashing entirely.
This proved to be a significant performance boost, since checking
containment and pushing items on the fringe happens A LOT.

** 1D array access projecting from 2 coords arithmetically appears slower then 2d array
Wrapped the byte arrays (currently) in a facade implememting an
IByteMap interface.  Better perf, also a simpler API to work with.

** Some function calls occur frequently enough to benefit from inlining
A little added noise, but definline useful 

** Some numerical ops benefit from explicit operators, e.g. == and unchecked stuff
Minor benefit but easy.  This wasn't crushing us, but helped on the
margins.  Datastructure access was far more important...

** Avoid RestFn invocation due to varargs arities
Numeric comparisons like <, when invoked with arities above 2, resort to 
a varargs implementation.  The idiom (< -1 x y) showed up in many places
on the hot path.  Every time you do this, you incur a small penalty,
as the destructured args reprsented by the xs as in (fn [x y & xs] ) 
are coerced into a seq and require some additional allocation and
function calls.  If you're on a hot path, and doing this in multiple
places, it adds up.  A quick fix, particularly for inlined calls (common
in this setting), is to just define a macro or equivalent inline function
that eliminates the need for varargs.
** Comparative version in Rust appears to not use compare solutions, so we don't
elided the call to compare solutions, available via [icfpc.main
solve-elided main-elided].

It "looks" like the Rust implementation is also far lighter in what it's
doing in some other areas (e.g. Thread checks and other stuff).  Likely
some more performance gains after doing a comparative review of the code (or 
better yet, just port the simpler Rust version using newfound tools and principles).

* Pending Efforts

** Load Level
I spent a decent amount of time exploring optimizations here, and implemented
many.  This isn't the lion's share of the performance cost though, but it's
on par with some of the bigger functions inside the bot namespace.  There are
likely additional optimizations.

** General Transition to Hinted Destructuring via with-slots
There are several functions I haven't transition to, that still
leverage the default get and nth implementations for destructuring
and ignore direct field access of level state.  I'm planning to
get them folded in and benchmark. 

** Verification of Results
To date, all of the transforms have been operational, rather than semantic.
The results "should" match the clojure baseline.  During testing, primarily
against the "big" problem set prob-277, I kept an eye on maintaining the
same scores throughout, and did catch one regression but fixed it.

Results currently check out when comparing the baseline scores.edn 
and the opt branch's scores.edn (each from respective run output.

I am confident the opt implementation is functionally equivalent to
the original baseline implementation.

** Additional performance
Aside from the basic mechanical changes (e.g. efficient reads), I
didn't alter anything algorithmically, since I didn't spend any time
reading about the original problem or comparing the implementation's
approach.  Consequently, there are likely additional areas to tweak,
introduce local mutation, or even leverage parallelism.  Currently the
distribution of work is effectively coarse-grained, on a per-problem
basis.  I suspect dissecting the work into finer-grained units for
integration with a producer/consumer queue (or a similar method) would
yield some nice results.  In other words, there may be additional
techniques that Clojure can explore to exploit available resources.

Some of the existing implementation is a product of experimentation.
The PooledFringe is - in hindsight - unnecessary if one just packs
along a search fringe with the level.  This allows workers to 
use (or re-use) the fringe instead of the current implementation
that maps threads to their own fringe via a concurrent hash map.
That introduces some needless overhead.

Another lateral would be going even more primitive in the data
representation and leveraging byte-buffer backed implementations
like tech.datatype or other struct-like libraries (Zach Tellman has
a couple).  These could open some interesting possibilites for
macros, cache local arrays, etc.  It would be an interesting
challenge to introduce psuedo value types via Clojure macros 
without losing expressiveness.


* Working Notes...
** Unexplained phenomena with unchecked math
I added some unchecked math ops early on prior to more thorough
profiling and exploration, which affected 2 functions in particular:
icfpc.level/valid-hand?  and icfpc.level/obstacle? .  

There was originally just one function (valid-hand?)  which I split
out into two during code spelunking (may reverse that decision in the
future).  It turns out that using unchecked math there for the
arithmetic actually introduced a regression, where the baseline
results did not match the opt branch.  I missed this regression (as it
happened before I locked things down for testing), but managed to find
and fix it prior to publishing this repo.  

So...word of wisdom: make sure your operations really can use
unchecked math :) I have no idea why in this case they don't (since
they're ostensibly integers...).

** dynamic var lookups
 (nth bots *bot*)
 
** rate function, explore*
** lots of destructuring
** advance*
*** ifpc.bot/explore*
**** rate
**** valid-hand?
The every? predicate for valid-hand invokes
ifpc.core/get-level 

get-level is primarily doing map lookups via
keyword lookup sites, could be an opportunity
to optimize.

coord->index is invoked a lot as well, causing
some overhead.

Lots of map-as-function invocations.
Lots of RestFN invocations surrounding < being
called on variadic args, leading to seq version
of next.

And not=, but does so against a var without
hinting, should be a number comparison.  Boxed
math..

**** clojure.lang.util/equiv
vector equality, pcequiv
tons of boxed numeric equality...
**** ifpc.core/get-level

**** ifpc.core/coord->idx
     

**** hashset.contains
**** ifpc.core.point/equals
uses map equality
invokes seq implementation
invokes point.size, which invokes clojure.lang.rt/count


**** ifpc.bot/step
    
**** self-time (invocation)
**** clojure.rt.count
**** keyword lookup site / get
**** hashset.add


*** ifpc.bot/act
*** ifpc.bot/move
**** ifpc.level/mark-wrapped
**** ifpc.level/extra-move
     
